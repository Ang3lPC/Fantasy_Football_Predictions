{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "\n",
    "from urllib.request import Request, urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_validate, cross_val_score, GridSearchCV,\\\n",
    "RandomizedSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from operator import itemgetter\n",
    "from math import sqrt\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Webscraping\n",
    "\n",
    "1. Here we use Request to get access to the desired website\n",
    "    - Side note, this took a little bit of time because I wasn't aware of headers and kept getting a 403 FORBIDDEN error. This video https://www.youtube.com/watch?v=6RfyXcf_vQo , helped me get through this\n",
    "2. Then we use urlopen to get the raw html code\n",
    "3. Finally we use BeautifulSoup to parse the html code and have it in a decipherable format\n",
    "4. Inspect the variable soup to make sure it's working!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.footballdb.com/teams/index.html'\n",
    "req = Request(url, headers={'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36'}) # Make a get request to retrieve the page\n",
    "webpage = urlopen(req).read()\n",
    "soup = BeautifulSoup(webpage, 'html.parser') # Pass the page contents to beautiful soup for parsing\n",
    "soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1.1 - Team url's\n",
    "\n",
    "We're going to use the soup from the previous part and extract each NFL team's url\n",
    "\n",
    "1. The urls are located in a table, so we have to find the table body 'tbody' and then find all the table rows 'tr' where they're located\n",
    "2. Unfortunately in all the '< a >' tags the href was only the latter part of the url\n",
    "    - ie. '/teams/nfl/buffalo-bills/stats'\n",
    "3. Once we have the latter part of the url, we add it to the base part of the url that every team website needs in order for us to later scrape information on each team.\n",
    "    - ie. 'https://www.footballdb.com' + '/teams/nfl/buffalo-bills/stats'\n",
    "4. Then we append it to a list that contains each teams url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Getting all the teams stats pages, and appending them to a list for the next step.\n",
    "\n",
    "Next step will involve going to each team's respective stats page and scraping each player ulr in passing, rushing, \n",
    "and receiving categories.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "teams = soup2.find('tbody').find_all('tr')\n",
    "\n",
    "url_part_one = 'https://www.footballdb.com'\n",
    "\n",
    "team_urls = []\n",
    "for team in teams:\n",
    "    if len(team.find_all('a')) > 1:\n",
    "        a_tags = team.find_all('a')[2]['href']\n",
    "        team_urls.append(url_part_one+a_tags)\n",
    "        \n",
    "team_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1.2 - Player url's\n",
    "\n",
    "We're using the list of team urls to scrape all of their players that have played offense url's\n",
    "\n",
    "1. In order to retrive each players url we have to do what we did in part 1 to pull and parse each team's html\n",
    "2. After we have the team's parsed html, we're going to find all of their tables that contain player stats\n",
    "    - This happens to be the first three tables! Passing, Rushing, Receiving.\n",
    "3. For each table we have to find all the 'span' tags that contained the 'hidden-xs' class which had each players url\n",
    "4. Within the variable sub_table we have all the 'hidden-xs', so for example in the Passing table for each 'hidden-xs' class we're going to do the same as part 1.1 and scrape the 'href' of each one\n",
    "5. We have the same problem as part 1.1 where the href is not complete, so before appending it to the list we have to add the base url and a specific ending so that we can just have their stats for the most recent year\n",
    "    - 'https://www.footballdb.com' + '/players/josh-allen-allenjo06' + '/gamelogs/2022'\n",
    "6. Then right before the function ends we have it pause for five seconds, so that the website doens't think anything fishy is going on and our IP does not get banned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_players_url_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def team_urls_to_player_list(list_of_team_urls):\n",
    "    url_part_one = 'https://www.footballdb.com'\n",
    "    url_part_two = '/gamelogs/2022'\n",
    "    \n",
    "    for team in list_of_team_urls:\n",
    "        \n",
    "        url = team\n",
    "        req = Request(url, headers={'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36'}) # Make a get request to retrieve the page\n",
    "        webpage = urlopen(req).read()\n",
    "        soup = BeautifulSoup(webpage, 'html.parser') # Pass the page contents to beautiful soup for parsing\n",
    "        \n",
    "        tables = soup.find_all('table')\n",
    "        \n",
    "        for i in range(3):\n",
    "            sub_table = tables[i].find_all('span', class_='hidden-xs')\n",
    "            for i in range(len(sub_table)):\n",
    "                player_url = sub_table[i].find('a')['href']\n",
    "                player_url = url_part_one+player_url+url_part_two\n",
    "                if player_url not in complete_players_url_list:\n",
    "                    complete_players_url_list.append(player_url)\n",
    "        print('Team retrieved')\n",
    "        complete_players_url_list\n",
    "        time.sleep(5)\n",
    "    print('All players urls retrieved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_urls_to_player_list(team_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking that all the players url's are there!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_players_url_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below is the process of iterating to see how to retrieve certain parts of the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.footballdb.com/teams/nfl/buffalo-bills/stats'\n",
    "req = Request(url, headers={'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36'}) # Make a get request to retrieve the page\n",
    "webpage = urlopen(req).read()\n",
    "soup = BeautifulSoup(webpage, 'html.parser') # Pass the page contents to beautiful soup for parsing\n",
    "tables = soup.find_all('table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables[0].find('span',{'class':'hidden-xs'}).find('a')['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables[0].find_all('span')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players_list = []\n",
    "url_part_one = 'https://www.footballdb.com'\n",
    "url_part_two = '/gamelogs/2022'\n",
    "\n",
    "for i in range(3):\n",
    "    sub_table = tables[i].find_all('span', class_='hidden-xs')\n",
    "    for i in range(len(sub_table)):\n",
    "        player_url = sub_table[i].find('a')['href']\n",
    "        player_url = url_part_one+player_url+url_part_two\n",
    "        if player_url not in players_list:\n",
    "            players_list.append(player_url)\n",
    "players_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterating on player's url to see how to extract certain information such as position\n",
    "- Not every player website had the same format, so I had to figure out a method that would work no matter the formatting\n",
    "- The solution was to only extract the text if the 'b' tag had the exact word 'Position'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url1 = 'https://www.footballdb.com/players/laquon-treadwell-treadla01/gamelogs/2022'\n",
    "req1 = Request(url1, headers={'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36'}) # Make a get request to retrieve the page\n",
    "webpage1 = urlopen(req1).read()\n",
    "soup1 = BeautifulSoup(webpage1, 'html.parser') # Pass the page contents to beautiful soup for parsing\n",
    "soup1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = soup1.find('div', id='playerbanner').find_all('b')\n",
    "\n",
    "[test[i].text for i in range(len(test))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test)):\n",
    "    if test[i].text == 'Position:':\n",
    "        a = test[i].next.next.strip()\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1.3 - Fetching row headers\n",
    "\n",
    "- Here we went into Josh Allen's page and retrieved the relevant row headers that will be used later on in a dataframe\n",
    "- Josh Allen had the relevant headers in his first three tables, so the code below won't work for all players url\n",
    "- The headers were in the 'header right' class, so in each 'header right' we find all the table headers 'th' and then we append each one as a str to the row_headers list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url1 = 'https://www.footballdb.com/players/josh-allen-allenjo06/gamelogs/2022'\n",
    "req1 = Request(url1, headers={'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36'}) # Make a get request to retrieve the page\n",
    "webpage1 = urlopen(req1).read()\n",
    "soup1 = BeautifulSoup(webpage1, 'html.parser') # Pass the page contents to beautiful soup for parsing\n",
    "\n",
    "jo_allen = soup1.find_all('table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_headers= []\n",
    "for i in range(3):\n",
    "    for x in jo_allen[i].find_all('tr', class_='header right'):\n",
    "        for y in x.find_all('th'):\n",
    "            row_headers.append(y.text)\n",
    "row_headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're inserting 'Name' and 'Pos' to the row_headers list because we know we're going to be scraping that for each\n",
    "# player as well.\n",
    "row_headers.insert(0,'Pos')\n",
    "row_headers.insert(0,'Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding out what index each ele is\n",
    "test_e = enumerate(row_headers, start=0)\n",
    "print(list(test_e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1.4 - Scraping stats for each player\n",
    "\n",
    "1. Here we created a function that takes a players soup and scrapes their name and position as well as all of their stats for Passing, Rushing, and Receiving\n",
    "2. We create four blank lists,\n",
    "    - player_stats to contain all of their game stats, ie. [[game1], [game2] ...]\n",
    "    - pass_stats for their passing stats for each game\n",
    "    - rush_stats for their rushing stats for each game\n",
    "    - rec_stats for their receiving stats for each game\n",
    "    \n",
    "3. First we scrape the name and position of the player and store those as separate variables\n",
    "4. Find  all the tables for the player\n",
    "5. In each table we find all the table headers, and make a list of them\n",
    "6. If the table header is Passing, Rushing, or Receiving then we go through each table row and find all the table data\n",
    "    - ie. each table row is one game and for each game we want all the table data which is the game's statistics\n",
    "7. If the player does not have one of the three aforementioned stats, then we populate that stat as 0's so that every game scraped will have the same total length\n",
    "    - This is very important for when we turn this list of lists into a dataframe\n",
    "8. Then at the end we combine each respective game's Passing, Rushing, and Receiving stat to the player_stats variable\n",
    "    - ie. [pass gm 1] + [rush gm 1] + [rec gm 1] -> append to player_stats, and do this for all their games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def players_table_to_stat_list(player_soup):\n",
    "    player_stats = []\n",
    "    pass_stats = [] #20 cols for players that have this table\n",
    "    rush_stats = [] #12 cols for players that have this table\n",
    "    rec_stats = [] #14 cols for players that have this table\n",
    "    \n",
    "    # Original code above, but have to change the way each week is inserted because not all players have the same\n",
    "    # table structure. ie. some have pass/rush/rec and some only have rush/rec, pass/rush, or etc.\n",
    "    \n",
    "    player_name = player_soup.find('div', class_='teamlabel').text\n",
    "    player_position = player_soup.find('div', id='playerbanner').find_all('b')\n",
    "    \n",
    "    for i in range(len(player_position)):\n",
    "        if player_position[i].text == 'Position:':\n",
    "            pos = player_position[i].next.next.strip()\n",
    "    player_position = pos        \n",
    "    \n",
    "    player_table = player_soup.find_all('table')\n",
    "    \n",
    "    for i in range(len(player_table)):\n",
    "        \n",
    "        player_th = player_table[i].find_all('th')\n",
    "        player_th = [x.text for x in player_th]\n",
    "            \n",
    "        # Passing Stats\n",
    "        if 'Passing' in player_th:\n",
    "            for x in player_table[i].find_all('tbody'):\n",
    "                for y in x.find_all('tr'):\n",
    "                    td_tags = y.find_all('td')\n",
    "                    if td_tags == []:\n",
    "                        continue\n",
    "                    else:\n",
    "                        pass\n",
    "                    td_val = [z.text for z in td_tags]\n",
    "                    pass_stats.append(td_val)\n",
    "\n",
    "        # Rushing Stats\n",
    "        elif 'Rushing' in player_th:    \n",
    "            for x in player_table[i].find_all('tbody'):\n",
    "                for y in x.find_all('tr'):\n",
    "                    td_tags = y.find_all('td')\n",
    "                    if td_tags == []:\n",
    "                        continue\n",
    "                    else:\n",
    "                        pass\n",
    "                    td_val = [z.text for z in td_tags]\n",
    "                    rush_stats.append(td_val)\n",
    "        \n",
    "        #Receiving Stats\n",
    "        elif 'Receiving' in player_th:    \n",
    "            for x in player_table[i].find_all('tbody'):\n",
    "                for y in x.find_all('tr'):\n",
    "                    td_tags = y.find_all('td')\n",
    "                    if td_tags == []:\n",
    "                        continue\n",
    "                    else:\n",
    "                        pass\n",
    "                    td_val = [z.text for z in td_tags]\n",
    "                    rec_stats.append(td_val)\n",
    "\n",
    "    #If player does not have specific table stats          \n",
    "    if pass_stats == []:\n",
    "        for x in player_table[i].find_all('tbody'):\n",
    "            for y in x.find_all('tr'):\n",
    "                pass_stats.append([0]*20)\n",
    "    \n",
    "    if rush_stats == []:\n",
    "        for x in player_table[i].find_all('tbody'):\n",
    "                        for y in x.find_all('tr'):\n",
    "                            rush_stats.append([0]*12)\n",
    "    if rec_stats == []:\n",
    "        for x in player_table[i].find_all('tbody'):\n",
    "                            for y in x.find_all('tr'):\n",
    "                                rec_stats.append([0]*14)\n",
    "   \n",
    "    \n",
    "    for throw in range(len(pass_stats)):\n",
    "        for rush in range(len(rush_stats)):\n",
    "            for rec in range(len(rec_stats)):\n",
    "                if throw == rush == rec:\n",
    "                    rush_stats[rush].extend(rec_stats[rec])\n",
    "                    pass_stats[throw].extend(rush_stats[rush])\n",
    "                    pass_stats[throw].insert(0,player_position)\n",
    "                    pass_stats[throw].insert(0,player_name)\n",
    "                    \n",
    "    player_stats.extend(pass_stats)\n",
    "    return player_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing it out and making sure the function works\n",
    "url2 = 'https://www.footballdb.com/players/laquon-treadwell-treadla01/gamelogs/2022'\n",
    "req2 = Request(url2, headers={'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36'}) # Make a get request to retrieve the page\n",
    "webpage2 = urlopen(req2).read()\n",
    "soup2 = BeautifulSoup(webpage2, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing it out and making sure the function works\n",
    "x = players_table_to_stat_list(soup2)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1.5 - Compiling every players stats\n",
    "\n",
    "Every player in complete_players_url_list will have their stats scraped and stored in the complete_players_stats variable\n",
    "\n",
    "1. We have a loop that goes through each player's url and parses there html, same steps as in Part 1.1\n",
    "2. Then the player's soup is fed into the function players_table_to_stat_list, and all of their games are stored into a variable called stats\n",
    "3. These stats are then extended into complete_players_stats, so that each list represents one game and not a set of games per player (as would be the case if we were to append)\n",
    "4. Then we have the function print out the url of the player that was just added, the amount of players scraped, and a little message letting us know it was succesful.\n",
    "5. Before moving to the next player we have the function pause for five seconds so that our IP does not get banned. This will take around an hour or two to fully run\n",
    "6. Once it's all done the function, let's us know via a print!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_players_stats = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players_url_retrieved = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def team_players_url_to_stats_table(players_url_list):\n",
    "    \n",
    "    \n",
    "    for player in players_url_list:\n",
    "        url = player\n",
    "        req = Request(url, headers={'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36'}) # Make a get request to retrieve the page\n",
    "        webpage = urlopen(req).read()\n",
    "        soup = BeautifulSoup(webpage, 'html.parser')\n",
    "        \n",
    "    \n",
    "        stats = players_table_to_stat_list(soup)\n",
    "        \n",
    "        complete_players_stats.extend(stats)\n",
    "        \n",
    "        print(url)\n",
    "        players_url_retrieved.append(url)\n",
    "        print(len(players_url_retrieved))\n",
    "        print('Player Stats added!')\n",
    "        print()\n",
    "        \n",
    "        time.sleep(5)\n",
    "    \n",
    "        \n",
    "    print('All Players added!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_players_url_to_stats_table(complete_players_url_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity checks, making sure that the amount of players scraped matches\n",
    "\n",
    "- Also checking the amount of games in the complete_players_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(complete_players_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(players_url_retrieved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(complete_players_url_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the length of the list is less than 48, then that means the player either did not play or was an irrelevant\n",
    "# summary statistic such as their totals for the season.\n",
    "# Testing random elements in the list\n",
    "print(len(complete_players_stats[22]))\n",
    "print(len(complete_players_stats[10578]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cleaning up the list to not include any list that does not have 48 elements/features.\n",
    "\"\"\"\n",
    "complete_players_stats_cleaned = [x for x in complete_players_stats if len(x) == 48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming that the clean list has reduced entries, and only games where the player played.\n",
    "print(len(complete_players_stats_cleaned))\n",
    "for i in complete_players_stats_cleaned:\n",
    "    if len(i) != 48:\n",
    "        print('error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the row headers again\n",
    "- Making sure that certain headers do not repeat and can be differenciated based on what stat it's a part of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_e = enumerate(row_headers, start=0)\n",
    "print(list(test_e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_headers[4] = 'Opp1'\n",
    "row_headers[24] = 'Opp2'\n",
    "row_headers[36] = 'Opp3'\n",
    "row_headers[9] = 'Pass_Yds'\n",
    "row_headers[11] = 'Pass_TD'\n",
    "row_headers[27] = 'Rush_Yds'\n",
    "row_headers[30] = 'Rush_TD'\n",
    "row_headers[39] = 'Rec_Yds'\n",
    "row_headers[42] = 'Rec_TD'\n",
    "row_headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1.6 - Making a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offense_nfl_df = pd.DataFrame(complete_players_stats_cleaned, columns=row_headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offense_nfl_df.to_csv('offense_nfl_2022.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
